# Author: Maha Shakir - Fixed Version
# Fixed by: Claude AI Assistant

# Import libraries/packages
import pickle
import re
import os
import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import nltk

# Download required NLTK data
try:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True) 
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
except:
    print("ØªØ­Ø°ÙŠØ±: Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NLTK ÙŠØ¯ÙˆÙŠØ§Ù‹")

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

# Configuration
show_plots = True
palette = sns.color_palette("Blues", 11)  # Updated for 11 categories

# Create sample dataset if original doesn't exist
def create_sample_dataset():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ"""
    
    categories = [
        'Pharmaceutical, Healthcare and Medical Sales',
        'Clinical Research', 
        'Pharmaceutical Marketing',
        'Manufacturing & Operations',
        'Science',
        'Medical Affairs / Pharmaceutical Physician',
        'Regulatory Affairs',
        'Medical Information and Pharmacovigilance', 
        'Data Management and Statistics',
        'Quality-assurance',
        'Pharmacy'
    ]
    
    sample_descriptions = [
        # Pharmaceutical Sales
        "pharmaceutical sales representative responsible for promoting medical products to healthcare professionals",
        "medical device sales specialist working with hospitals and clinics",
        "healthcare sales manager overseeing regional pharmaceutical product distribution",
        
        # Clinical Research
        "clinical research coordinator managing patient recruitment and trial protocols", 
        "clinical data manager responsible for collecting and analyzing trial results",
        "research scientist conducting pharmaceutical clinical studies",
        
        # Pharmaceutical Marketing
        "pharmaceutical marketing manager developing promotional strategies for new drugs",
        "medical marketing specialist creating educational materials for healthcare providers",
        "brand manager for pharmaceutical products and market analysis",
        
        # Manufacturing & Operations  
        "manufacturing engineer optimizing pharmaceutical production processes",
        "operations manager overseeing drug manufacturing facilities",
        "production supervisor ensuring quality manufacturing standards",
        
        # Science
        "research scientist developing new pharmaceutical compounds", 
        "laboratory technician conducting chemical analysis and testing",
        "biochemist studying drug interactions and molecular mechanisms",
        
        # Medical Affairs
        "medical affairs specialist providing scientific support for pharmaceutical products",
        "pharmaceutical physician reviewing clinical data and medical literature", 
        "medical science liaison communicating with key opinion leaders",
        
        # Regulatory Affairs
        "regulatory affairs specialist ensuring FDA compliance for drug approvals",
        "regulatory manager preparing submissions for health authorities",
        "compliance officer managing pharmaceutical regulations and guidelines",
        
        # Medical Information  
        "medical information specialist responding to healthcare provider inquiries",
        "pharmacovigilance associate monitoring drug safety and adverse events",
        "drug safety officer analyzing safety data and risk assessments",
        
        # Data Management
        "biostatistician analyzing clinical trial statistical data",
        "data manager ensuring quality and integrity of clinical databases", 
        "statistical programmer developing analysis plans for studies",
        
        # Quality Assurance
        "quality assurance specialist ensuring GMP compliance in manufacturing",
        "QA manager overseeing quality control processes and audits",
        "quality control analyst testing pharmaceutical products for specifications",
        
        # Pharmacy
        "clinical pharmacist providing medication therapy management",
        "hospital pharmacist dispensing medications and counseling patients",
        "pharmaceutical care specialist optimizing drug therapy outcomes"
    ]
    
    # Create balanced dataset
    data = []
    descriptions_per_category = len(sample_descriptions) // len(categories)
    
    for i, category in enumerate(categories):
        start_idx = i * descriptions_per_category
        end_idx = start_idx + descriptions_per_category
        
        for desc in sample_descriptions[start_idx:end_idx]:
            data.append({'job_description': desc, 'category': category})
        
        # Add extra descriptions if available
        if i < len(sample_descriptions) % len(categories):
            extra_idx = len(categories) * descriptions_per_category + i
            if extra_idx < len(sample_descriptions):
                data.append({'job_description': sample_descriptions[extra_idx], 'category': category})
    
    df = pd.DataFrame(data)
    df.to_csv('dataset0.csv', index=False)
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ù€ {len(df)} Ø¹ÙŠÙ†Ø©")
    return df

# Load the dataset
try:
    if not os.path.exists('dataset0.csv'):
        print("âš ï¸  Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
        dataset = create_sample_dataset()
    else:
        dataset = pd.read_csv("dataset0.csv", on_bad_lines='skip')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    print("Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
    dataset = create_sample_dataset()

# Display dataset information
print("\n=== Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===")
print(f"Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {dataset.shape}")
print(f"Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {list(dataset.columns)}")
print(f"Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {dataset['category'].nunique()}")

print("\n=== Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===")
print(dataset.head())

print("\n=== ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª ===")
print(dataset['category'].value_counts())

# Create category mapping
target_category = dataset['category'].unique()
dataset['categoryId'] = dataset['category'].factorize()[0]

category_df = dataset[['category', 'categoryId']].drop_duplicates().sort_values('categoryId')
category_to_id = dict(category_df.values)
id_to_category = dict(category_df[['categoryId', 'category']].values)

print("\n=== ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙØ¦Ø§Øª ===")
for cat_id, cat_name in id_to_category.items():
    print(f"{cat_id}: {cat_name}")

# Visualization
if show_plots:
    plt.figure(figsize=(15, 8))
    counts = dataset['category'].value_counts()
    colors = sns.color_palette("Blues", len(counts))
    
    ax = counts.plot(kind="bar", color=colors, figsize=(15, 8))
    plt.title("ØªÙˆØ²ÙŠØ¹ ÙØ¦Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù", fontsize=16, pad=20)
    plt.xlabel("ÙØ¦Ø© Ø§Ù„ÙˆØ¸ÙŠÙØ©", fontsize=12)
    plt.ylabel("Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù", fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('category_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Pie chart
    plt.figure(figsize=(12, 10))
    plt.pie(counts.values, labels=counts.index, autopct="%1.1f%%",
            colors=palette[:len(counts)], startangle=45)
    plt.title("Ù†Ø³Ø¨ ÙØ¦Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù", fontsize=16, pad=20)
    plt.axis('equal')
    plt.tight_layout()
    plt.savefig('category_pie_chart.png', dpi=300, bbox_inches='tight')
    plt.show()

# NLP Processing Functions
def remove_tags(text):
    """Ø¥Ø²Ø§Ù„Ø© HTML tags"""
    if pd.isna(text):
        return ""
    remove = re.compile(r'<.*?>')
    return re.sub(remove, '', str(text))

def special_char(text):
    """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©"""
    if pd.isna(text):
        return ""
    result = ''
    for char in str(text):
        if char.isalnum():
            result += char
        else:
            result += ' '
    return result

def convert_lower(text):
    """ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©"""
    if pd.isna(text):
        return ""
    return str(text).lower()

def remove_stopwords(text):
    """Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚Ù"""
    if pd.isna(text):
        return []
    try:
        stop_words = set(stopwords.words('english'))
        words = word_tokenize(str(text))
        return [word for word in words if word not in stop_words and len(word) > 2]
    except:
        return str(text).split()

def lemmatize_word(words):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø°ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    if not words:
        return ""
    try:
        wordnet = WordNetLemmatizer()
        if isinstance(words, list):
            return " ".join([wordnet.lemmatize(word) for word in words])
        else:
            return str(words)
    except:
        if isinstance(words, list):
            return " ".join(words)
        return str(words)

# Apply preprocessing
print("\n=== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ===")
print("1. Ø¥Ø²Ø§Ù„Ø© HTML tags...")
dataset['job_description'] = dataset['job_description'].apply(remove_tags)

print("2. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ©...")
dataset['job_description'] = dataset['job_description'].apply(special_char)

print("3. ØªØ­ÙˆÙŠÙ„ Ù„Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©...")
dataset['job_description'] = dataset['job_description'].apply(convert_lower)

print("4. Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙˆÙ‚Ù...")
dataset['job_description'] = dataset['job_description'].apply(remove_stopwords)

print("5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø°ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª...")
dataset['job_description'] = dataset['job_description'].apply(lemmatize_word)

print("âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø¨Ù†Ø¬Ø§Ø­")

# Feature extraction
print("\n=== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª ===")
x = dataset['job_description']
y = dataset['category']

# TF-IDF Vectorization
max_features = min(5000, len(dataset) * 2)  # Adjust for small datasets
features_extractor = TfidfVectorizer(
    max_features=max_features,
    min_df=1,
    max_df=0.9,
    ngram_range=(1, 2)
)

X_features = features_extractor.fit_transform(x).toarray()
print(f"Ø´ÙƒÙ„ Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª: {X_features.shape}")

# Train-test split
test_size = min(0.3, 0.5)  # Adjust for small datasets
X_train, X_test, y_train, y_test = train_test_split(
    X_features, y, test_size=test_size, random_state=42, 
    shuffle=True, stratify=y
)

print(f"Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {X_train.shape[0]} Ø¹ÙŠÙ†Ø©")
print(f"Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {X_test.shape[0]} Ø¹ÙŠÙ†Ø©")

# Model training and evaluation
performance_list = []

def run_model(model_name):
    """ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    print(f"\nğŸ”„ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬: {model_name}")
    
    try:
        # Model selection
        if model_name == 'Logistic Regression':
            model = LogisticRegression(max_iter=1000, random_state=42)
        elif model_name == 'Random Forest':
            model = RandomForestClassifier(n_estimators=100, random_state=42)
        elif model_name == 'Multinomial Naive Bayes':
            model = MultinomialNB(alpha=1.0)
        elif model_name == 'Support Vector Classifier':
            model = SVC(probability=True, random_state=42)
        elif model_name == 'Decision Tree':
            model = DecisionTreeClassifier(random_state=42)
        elif model_name == 'K Nearest Neighbors':
            model = KNeighborsClassifier(n_neighbors=min(5, len(y_train)//2))
        elif model_name == 'Gaussian Naive Bayes':
            model = GaussianNB()
        
        # Train model
        classifier = OneVsRestClassifier(model)
        classifier.fit(X_train, y_train)
        
        # Predictions
        y_pred_train = classifier.predict(X_train)
        y_pred_test = classifier.predict(X_test)
        
        # Evaluation
        for data, pred, data_name in [(y_train, y_pred_train, 'Training'), 
                                      (y_test, y_pred_test, 'Testing')]:
            accuracy = accuracy_score(data, pred) * 100
            precision, recall, f1score, _ = score(data, pred, average='weighted')
            
            print(f"  {data_name} - Ø¯Ù‚Ø©: {accuracy:.2f}%")
            print(f"  {data_name} - Ø¯Ù‚Ø©: {precision:.3f}, Ø§Ø³ØªØ±Ø¬Ø§Ø¹: {recall:.3f}, F1: {f1score:.3f}")
            
            # Confusion matrix
            if show_plots and data_name == 'Testing':
                plt.figure(figsize=(10, 8))
                cm = confusion_matrix(data, pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                           xticklabels=classifier.classes_,
                           yticklabels=classifier.classes_)
                plt.title(f'Ù…ØµÙÙˆÙØ© Ø§Ù„Ø®Ù„Ø· - {model_name}')
                plt.ylabel('Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©')
                plt.xlabel('Ø§Ù„ÙØ¦Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©')
                plt.xticks(rotation=45)
                plt.yticks(rotation=0)
                plt.tight_layout()
                plt.savefig(f'{model_name.replace(" ", "_")}_confusion_matrix.png', 
                           dpi=300, bbox_inches='tight')
                plt.show()
            
            # Store performance
            performance_list.append({
                'Model': model_name,
                'Data': data_name,
                'Accuracy': round(accuracy, 2),
                'Precision': round(precision, 3),
                'Recall': round(recall, 3),
                'F1': round(f1score, 3)
            })
        
        return classifier
    
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {model_name}: {e}")
        return None

# Train all models
print("\n=== ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ===")
models = [
    'Logistic Regression',
    'Random Forest', 
    'Multinomial Naive Bayes',
    'Decision Tree',
    'K Nearest Neighbors'
]

trained_models = {}
for model_name in models:
    model = run_model(model_name)
    if model is not None:
        trained_models[model_name] = model

# Performance comparison
print("\n=== Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ===")
if performance_list:
    performance_df = pd.DataFrame(performance_list)
    test_performance = performance_df[performance_df['Data'] == 'Testing']
    
    print("\nØ£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
    print(test_performance[['Model', 'Accuracy', 'Precision', 'Recall', 'F1']].to_string(index=False))
    
    # Find best model
    best_model_name = test_performance.loc[test_performance['Accuracy'].idxmax(), 'Model']
    best_accuracy = test_performance['Accuracy'].max()
    
    print(f"\nğŸ† Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model_name} Ø¨Ø¯Ù‚Ø© {best_accuracy}%")
    
    # Save best model
    if best_model_name in trained_models:
        best_model = trained_models[best_model_name]
        
        try:
            with open("best_classifier.pkl", "wb") as f:
                pickle.dump(best_model, f)
            
            with open("features_extractor.pkl", "wb") as f:
                pickle.dump(features_extractor, f)
            
            with open("category_mapping.pkl", "wb") as f:
                pickle.dump(id_to_category, f)
            
            print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")

# Interactive testing
def predict_job_category(job_description, model, vectorizer):
    """ØªØµÙ†ÙŠÙ ÙˆØµÙ ÙˆØ¸ÙŠÙØ© Ø¬Ø¯ÙŠØ¯"""
    try:
        # Preprocess
        processed = remove_tags(job_description)
        processed = special_char(processed)
        processed = convert_lower(processed)
        processed = remove_stopwords(processed)
        processed = lemmatize_word(processed)
        
        # Transform and predict
        features = vectorizer.transform([processed])
        prediction = model.predict(features)[0]
        
        # Get confidence if possible
        try:
            proba = model.predict_proba(features)
            confidence = max(proba[0]) * 100
        except:
            confidence = 0
        
        return prediction, confidence
    
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ: {e}")
        return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯", 0

# Test with examples
print("\n=== Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ===")
if trained_models and best_model_name in trained_models:
    test_examples = [
        "clinical research coordinator managing patient trials",
        "pharmaceutical sales representative promoting medical devices",
        "quality assurance specialist ensuring manufacturing compliance",
        "marketing manager for pharmaceutical products"
    ]
    
    best_model = trained_models[best_model_name]
    
    for i, example in enumerate(test_examples, 1):
        category, confidence = predict_job_category(example, best_model, features_extractor)
        print(f"{i}. Ø§Ù„ÙˆØµÙ: {example}")
        print(f"   Ø§Ù„ØªØµÙ†ÙŠÙ: {category} (Ø«Ù‚Ø©: {confidence:.1f}%)")
        print()

# Interactive loop
print("\n=== Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ ===")
if trained_models and best_model_name in trained_models:
    while True:
        user_input = input("\nØ£Ø¯Ø®Ù„ ÙˆØµÙ Ø§Ù„ÙˆØ¸ÙŠÙØ© (Ø£Ùˆ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬): ")
        if user_input.lower() in ['exit', 'quit', 'Ø®Ø±ÙˆØ¬']:
            break
        
        if user_input.strip():
            category, confidence = predict_job_category(user_input, best_model, features_extractor)
            print(f"Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {category}")
            print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {confidence:.1f}%")

print("\nâœ… Ø§ÙƒØªÙ…Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø¬Ø§Ø­!")
print("ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©:")
print("   - best_classifier.pkl (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙØ¶Ù„)")  
print("   - features_extractor.pkl (Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ)")
print("   - category_mapping.pkl (ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙØ¦Ø§Øª)")
print("   - Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© (.png files)")